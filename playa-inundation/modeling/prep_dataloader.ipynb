{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import json\n",
    "import glob\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some params\n",
    "categorical_cols = ['id', 'huc8', 'author']\n",
    "n_cat_cols = len(categorical_cols)\n",
    "n_num_cols = 26 # Hard coded, if you add more you'll have to change this\n",
    "hdf_path = './all_prepped_data.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(df):\n",
    "    ind_year = np.where(np.array(df.index.names)=='year')[0][0]\n",
    "    train_df = df.loc[df.index.get_level_values(ind_year)<=2010]\n",
    "    val_df = df.loc[(df.index.get_level_values(ind_year)>2010) & (df.index.get_level_values(ind_year)<=2014)]\n",
    "    test_df = df.loc[df.index.get_level_values(ind_year)>2014]\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def split_by_playa(x, x_cat, y, seq_length):\n",
    "    seq_starts = np.arange(0, x.shape[0], seq_length)\n",
    "    x_arr = np.array([x[i:(i+seq_length)] for i in seq_starts])\n",
    "    x_cat_arr = np.array([x_cat[i:(i+seq_length)] for i in seq_starts])\n",
    "    y_arr = np.array([y[i:(i+seq_length)] for i in seq_starts])\n",
    "    return x_arr,  x_cat_arr, y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_lc_frac_df(ids=[]):\n",
    "    \"\"\"LC Frac csv is hardcoded! Change if you need it\"\"\"\n",
    "    lc_df = pd.read_csv('../data/fraster_landcover_allyears_bigger.csv').set_index('id')\n",
    "    if len(ids)>0:\n",
    "        lc_df = lc_df.loc[ids]\n",
    "    lc_frac = pd.DataFrame()\n",
    "    for col in lc_df.columns:\n",
    "        year = int(col[0:4])\n",
    "        jsond = lc_df[col].str.replace(r'([0-9]+)(:)', r'\"\\1\"\\2', regex=True).apply(json.loads)\n",
    "        temp_frac_df = (pd.json_normalize(jsond)/5000)\n",
    "        temp_frac_df.columns = ['lcf{}'.format(lc) for lc in temp_frac_df.columns]\n",
    "        temp_frac_df = temp_frac_df.assign(id=lc_df.index, year=year)\n",
    "        lc_frac = lc_frac.append(temp_frac_df)\n",
    "    \n",
    "    # Fill in rest of columns (if there's no playas with some LC in this county)\n",
    "    all_lc_cols = np.array(['lcf11', 'lcf13', 'lcf12', 'lcf9', 'lcf2', 'lcf6', 'lcf1', 'lcf14',\n",
    "                   'lcf7', 'lcf15', 'lcf16', 'lcf8', 'lcf10', 'lcf3'])\n",
    "    for col in all_lc_cols[~np.isin(all_lc_cols, lc_frac.columns)]:\n",
    "        lc_frac[col] = 0   \n",
    "    lc_frac.fillna(0,inplace=True)\n",
    "    \n",
    "    return lc_frac.set_index(['id','year'])\n",
    "\n",
    "\n",
    "def read_join_csv(inun_csv, drop_zeros=True):\n",
    "    # Prep inundation data\n",
    "    inun_df = pd.read_csv(inun_csv)\n",
    "    inun_df.set_index(['id','year','month'], inplace=True)\n",
    "    \n",
    "    # If playa overlapped HUCs, got duplicates. This removes them: \n",
    "    if inun_df.index.get_level_values(0).unique().shape[0] != inun_df.shape[0]/418:\n",
    "        inun_df = inun_df.groupby(level=[0,1,2]).first()\n",
    "        \n",
    "    # If we don't want zero-inundation playas, drop them here\n",
    "    if drop_zeros:\n",
    "        max_inun = inun_df.groupby('id').agg({'inundation':'max'})\n",
    "        zero_ids = max_inun.loc[max_inun['inundation']==0].index\n",
    "        inun_df.drop(zero_ids, inplace=True)\n",
    "        if inun_df.shape[0]==0:\n",
    "            return \n",
    "        \n",
    "    # Prep weather data\n",
    "    weather_csv = inun_csv.replace('inun_frac_','weather_')\n",
    "    weather_df = pd.read_csv(weather_csv)\n",
    "    weather_df.set_index(['id','year','month'], inplace=True)\n",
    "    \n",
    "    # If playa overlapped HUCs, got duplicates. This removes them: \n",
    "    if weather_df.index.get_level_values(0).unique().shape[0] != weather_df.shape[0]/418:\n",
    "        weather_df = weather_df.groupby(level=[0,1,2]).first()\n",
    "        \n",
    "    joined_df = weather_df.join(inun_df, how='inner')\n",
    "                               \n",
    "    if joined_df.index.get_level_values(0).unique().shape[0] != joined_df.shape[0]/418:\n",
    "        print('still not')\n",
    "\n",
    "    \n",
    "    # Finally, prep landcover fraction dataframe\n",
    "    # Both prep and join are a bit slow\n",
    "    # Could prep into fractions ahead of time\n",
    "    # And also split up lc df by county\n",
    "    lc_frac_df = prep_lc_frac_df(ids=joined_df.index.get_level_values(0).unique())\n",
    "    joined_df = joined_df.join(lc_frac_df, how='inner')\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "\n",
    "def read_clean_county(csv, binarize=True):\n",
    "    traj = read_join_csv(csv, drop_zeros=False)\n",
    "    traj.fillna(0, inplace=True)\n",
    "    \n",
    "    # Drop area (which is acres*inundation)\n",
    "    traj.drop(columns=['area'], inplace=True)\n",
    "\n",
    "    # Set ID as another column\n",
    "    traj['id'] = traj.index.get_level_values(0)\n",
    "    \n",
    "    # Pop inundation to the end\n",
    "    inun = traj.pop('inundation')\n",
    "    if binarize:\n",
    "        traj['inundation'] = (inun > 0).astype(int)\n",
    "    else:\n",
    "        traj['inundation'] = inun\n",
    "    \n",
    "\n",
    "    return traj\n",
    "\n",
    "\n",
    "def split_x_y(df, cat_cols):\n",
    "    x, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    return x.drop(columns=cat_cols), x[cat_cols], y\n",
    "\n",
    "\n",
    "def fill_nas(train, val, test):\n",
    "    \"\"\"sthick2013 has NAs, recorded as -9999. So does windevyr (as 9999).\n",
    "    Here we fill with median values from the county, or if unavailable from whole dataset\"\"\"\n",
    "    # Fill sthick 13\n",
    "    sthick_full_median = 72.03190000000001\n",
    "    if (train['sthick2013']>-9999).sum() > 0:\n",
    "        sthick_fill = np.median(train.loc[train['sthick2013']>-9999, 'sthick2013'])\n",
    "    else:\n",
    "        sthick_fill = sthick_full_median\n",
    "        \n",
    "    train.loc[train['sthick2013']==-9999, 'sthick2013'] = sthick_fill\n",
    "    val.loc[val['sthick2013']==-9999, 'sthick2013'] = sthick_fill\n",
    "    test.loc[test['sthick2013']==-9999, 'sthick2013'] = sthick_fill\n",
    "    \n",
    "    # Fill winddevyr\n",
    "    winddevyr_full_median = 2015\n",
    "    if (train['winddevyr']<9999).sum() > 0:\n",
    "        winddevyr_fill = np.median(train.loc[train['winddevyr']<9999, 'winddevyr'])\n",
    "    else:\n",
    "        winddevyr_fill = winddevyr_full_median\n",
    "    train.loc[train['winddevyr']==9999, 'winddevyr'] = winddevyr_fill\n",
    "    val.loc[val['winddevyr']==9999, 'winddevyr'] = winddevyr_fill\n",
    "    test.loc[test['winddevyr']==9999, 'winddevyr'] = winddevyr_fill\n",
    "    \n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H5 stuff\n",
    "def create_h5_dsets(path, n_num_features, n_cat_features):\n",
    "    with h5py.File(path, \"a\") as f:\n",
    "        f.create_dataset('train_num', (0, 322,n_num_features), maxshape=(None,322,n_num_features))\n",
    "        f.create_dataset('train_cat', (0, 322,n_cat_features), maxshape=(None,322,n_cat_features))\n",
    "        f.create_dataset('train_y', (0, 322), maxshape=(None,322))\n",
    "        \n",
    "        f.create_dataset('val_num', (0, 48,n_num_features), maxshape=(None,48,n_num_features))\n",
    "        f.create_dataset('val_cat', (0, 48,n_cat_features), maxshape=(None,48,n_cat_features))\n",
    "        f.create_dataset('val_y', (0, 48), maxshape=(None,48))\n",
    "\n",
    "        f.create_dataset('test_num', (0, 48,n_num_features), maxshape=(None,48,n_num_features))\n",
    "        f.create_dataset('test_cat', (0, 48,n_cat_features), maxshape=(None,48,n_cat_features))\n",
    "        f.create_dataset('test_y', (0, 48), maxshape=(None,48))\n",
    "        \n",
    "\n",
    "def append_to_dset(hdf_fh, dset_key, array):\n",
    "    dset = hdf_fh[dset_key]\n",
    "    dset.resize(dset.shape[0]+array.shape[0], axis=0)\n",
    "    dset[-array.shape[0]:] = array\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def save_set_hdf(X_num, X_cat, y, dset_prefix, path):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X_num (pd.DataFrame) =  Numerical features\n",
    "        X_cat (pd.DataFrame) = Categorical features for embeddings\n",
    "        y (pd.Series) = Target (inundation)\n",
    "        dset_prefix (str) = prefix for HDF5 dataset (train, val, or test)\n",
    "    \"\"\"\n",
    "    \n",
    "    seq_length = X_num.loc[X_num.index.get_level_values(0)[0]].shape[0]\n",
    "    X_num_ar, X_cat_ar, y_ar = split_by_playa(\n",
    "        X_num.values, X_cat.values, y.values, seq_length=seq_length\n",
    "    )\n",
    "    \n",
    "    with h5py.File(path, \"a\") as f:\n",
    "        append_to_dset(f, '{}_num'.format(dset_prefix), X_num_ar)\n",
    "        append_to_dset(f, '{}_cat'.format(dset_prefix), X_cat_ar)\n",
    "        append_to_dset(f, '{}_y'.format(dset_prefix), y_ar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_read_append_to_hdf(csv, hdf_path, categorical_cols):\n",
    "    traj = read_clean_county(csv)\n",
    "    \n",
    "    train, val, test = split_train_test_val(traj)\n",
    "    \n",
    "    # Split into features and target\n",
    "    train_X_num, train_X_cat, train_y = split_x_y(train, categorical_cols)\n",
    "    val_X_num, val_X_cat, val_y = split_x_y(val, categorical_cols)\n",
    "    test_X_num, test_X_cat, test_y = split_x_y(test, categorical_cols)\n",
    "\n",
    "    # Fill NAs\n",
    "    train_X_num, val_X_num, test_X_num = fill_nas(train_X_num, val_X_num, test_X_num)\n",
    "    \n",
    "    # Save each set to the hdf5 file\n",
    "    save_set_hdf(train_X_num, train_X_cat, train_y, 'train', hdf_path)\n",
    "    save_set_hdf(val_X_num, val_X_cat, val_y, 'val', hdf_path)\n",
    "    save_set_hdf(test_X_num, test_X_cat, test_y, 'test', hdf_path)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_h5_dsets(hdf_path, n_num_cols, n_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inun_csv_list =  glob.glob('../data/state_county_csvs/counties/inun_frac*')\n",
    "for cur_csv in inun_csv_list:\n",
    "    full_read_append_to_hdf(cur_csv, hdf_path, categorical_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playa_venv",
   "language": "python",
   "name": "playa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
