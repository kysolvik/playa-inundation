{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Robert Guthrie https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "# And: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import json\n",
    "import zoib\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(df):\n",
    "    ind_year = np.where(np.array(traj.index.names)=='year')[0][0]\n",
    "    train_df = df.loc[df.index.get_level_values(ind_year)<=2010]\n",
    "    val_df = df.loc[(df.index.get_level_values(ind_year)>2010) & (df.index.get_level_values(ind_year)<=2014)]\n",
    "    test_df = df.loc[df.index.get_level_values(ind_year)>2014]\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_lc_frac_df(ids=[]):\n",
    "    \"\"\"LC Frac csv is hardcoded! Change if you need it\"\"\"\n",
    "    lc_df = pd.read_csv('../data/fraster_landcover_allyears_bigger.csv').set_index('id')\n",
    "    if len(ids)>0:\n",
    "        lc_df = lc_df.loc[ids]\n",
    "    lc_frac = pd.DataFrame()\n",
    "    for col in lc_df.columns:\n",
    "        year = int(col[0:4])\n",
    "        jsond = lc_df[col].str.replace(r'([0-9]+)(:)', r'\"\\1\"\\2', regex=True).apply(json.loads)\n",
    "        temp_frac_df = (pd.json_normalize(jsond)/5000)\n",
    "        temp_frac_df.columns = ['lcf{}'.format(lc) for lc in temp_frac_df.columns]\n",
    "        temp_frac_df = temp_frac_df.assign(id=lc_df.index, year=year)\n",
    "        lc_frac = lc_frac.append(temp_frac_df)\n",
    "    lc_frac.fillna(0,inplace=True)\n",
    "    \n",
    "    return lc_frac.set_index(['id','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_join_csv(inun_csv, drop_zeros=True):\n",
    "    # Prep inundation data\n",
    "    inun_df = pd.read_csv(inun_csv)\n",
    "    inun_df.set_index(['id','year','month'], inplace=True)\n",
    "    inun_df = inun_df.loc[~inun_df['inundation'].isna()]\n",
    "    if drop_zeros:\n",
    "        max_inun = inun_df.groupby('id').agg({'inundation':'max'})\n",
    "        zero_ids = max_inun.loc[max_inun['inundation']==0].index\n",
    "        inun_df.drop(zero_ids, inplace=True)\n",
    "        if inun_df.shape[0]==0:\n",
    "            return \n",
    "        \n",
    "    # Prep weather data\n",
    "    weather_csv = inun_csv.replace('inun_frac_','weather_')\n",
    "    weather_df = pd.read_csv(weather_csv)\n",
    "    weather_df.set_index(['id','year','month'], inplace=True)\n",
    "    joined_df = weather_df.join(inun_df, how='inner')\n",
    "    \n",
    "    # Finally, prep landcover fraction dataframe\n",
    "    # Both prep and join are a bit slow\n",
    "    # Could prep into fractions ahead of time\n",
    "    # And also split up lc df by county\n",
    "    lc_frac_df = prep_lc_frac_df(ids=joined_df.index.get_level_values(0).unique())\n",
    "    joined_df = joined_df.join(lc_frac_df, how='inner')\n",
    "    \n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_num_playas = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inun_csv_list = glob.glob('../data/state_county_csvs/counties/inun_frac*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.DataFrame()\n",
    "while joined_df.index.get_level_values(0).unique().shape[0] <= target_num_playas:\n",
    "    rand_csv = np.random.choice(inun_csv_list)\n",
    "    joined_df = pd.concat([joined_df, read_join_csv(rand_csv, drop_zeros=True)])\n",
    "    \n",
    "joined_df.fillna(0, inplace=True)\n",
    "\n",
    "joined_df = joined_df.loc[joined_df.index.get_level_values(0).unique()[:target_num_playas]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = joined_df\n",
    "traj = traj.drop(columns=['area'])#[['inundation', 'acres', 'vpd', 'temp','precip']]\n",
    "n_features = traj.shape[1]\n",
    "traj['inundation'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_timeseries = 418\n",
    "new_ids = np.array([\n",
    "    np.repeat(i, len_of_timeseries) for i in range(int(traj.shape[0]/len_of_timeseries))]\n",
    ").flatten()\n",
    "traj = traj.assign(id=new_ids) # Put id at end for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop inundation to end\n",
    "inun = traj.pop('inundation')\n",
    "traj['inundation'] = inun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params to set\n",
    "hidden_dim = 128\n",
    "embedding_dim = 1\n",
    "\n",
    "num_layers=1\n",
    "learning_rate = 0.05\n",
    "num_epochs = 2000\n",
    "weight_decay = 0\n",
    "lr_gamma = 0.95\n",
    "lr_decay_step_size =25\n",
    "regularization_weight = 0\n",
    "batch_size = 1\n",
    "\n",
    "early_stopping=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_id(x, y):\n",
    "    seq_length = np.sum((x[:,-1]==x[0,-1]))\n",
    "    seq_starts = np.arange(0, x.shape[0], seq_length)\n",
    "    x_arr = np.array([np.array(x[i:(i+seq_length)]) for i in seq_starts])\n",
    "    y_arr = np.array([np.array(y[i:(i+seq_length)]) for i in seq_starts])\n",
    "    return x_arr, y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train, val, test = split_train_test_val(traj)\n",
    "train_X, train_y = train.values[:, :-1], train.values[:, -1]\n",
    "val_X, val_y = val.values[:, :-1], val.values[:, -1]\n",
    "test_X, test_y = test.values[:, :-1], test.values[:, -1]\n",
    "# Run scaler, but not on ID\n",
    "train_X[:,:-1] = scaler.fit_transform(train_X[:,:-1])\n",
    "val_X[:,:-1] = scaler.transform(val_X[:,:-1])\n",
    "test_X[:,:-1] = scaler.transform(test_X[:,:-1])\n",
    "\n",
    "num_playas = int(train_X[:, -1].max())+1\n",
    "lstm_input_size = traj.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, val, and test sets\n",
    "train_X_array, train_y = split_by_id(train_X, train_y)\n",
    "val_X_array, val_y = split_by_id(val_X, val_y)\n",
    "test_X_array, test_y = split_by_id(test_X, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torch.utils.data.TensorDataset(torch.Tensor(train_X_array), torch.Tensor(train_y),\n",
    "                                          torch.Tensor(val_X_array), torch.Tensor(val_y))\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, num_playas, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_playas = num_playas\n",
    "\n",
    "        # Define embedding layer\n",
    "        self.embedding = nn.Embedding(self.num_playas, self.embedding_dim)\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim + self.embedding_dim - 1, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "\n",
    "        # Define activations for output\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.exp = torch.exp\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of input: [batch_size, timesteps, input_dims]\n",
    "        # shape of lstm_out: [batch_size, timesteps, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        # Shape of y_pred: [batch_size, timesteps, 4]\n",
    "\n",
    "        # Run ids through embedding layer\n",
    "        self.emb_layer = self.embedding(input[:,:,-1].long())\n",
    "        \n",
    "        # Concat and run through LSTM\n",
    "        lstm_out, self.hidden = self.lstm(torch.cat((input[:,:,:-1],self.emb_layer), 2))\n",
    "\n",
    "        \n",
    "        # Run activation and get outputs\n",
    "        lin_act = self.linear(lstm_out)\n",
    "        y_pred = torch.cat((self.sigmoid(lin_act[:,:,0:2]),torch.exp(lin_act[:,:,2:4])), 2)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = LSTM(input_dim = lstm_input_size,\n",
    "             embedding_dim=embedding_dim,\n",
    "             num_playas=num_playas,\n",
    "             hidden_dim=hidden_dim,\n",
    "             batch_size=batch_size,\n",
    "             output_dim=4,\n",
    "             num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = nn.L1Loss()\n",
    "    \n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=lr_decay_step_size, gamma=lr_gamma)\n",
    "#####################---------------------------------------------------------------------------\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "best_loss = 1000\n",
    "for t in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    val_epoch_loss = 0\n",
    "    total_items = 0\n",
    "    val_total_items = 0\n",
    "    # List to store all predictions\n",
    "    all_train_pred = []\n",
    "    all_val_pred = []\n",
    "    for x_batch, y_batch, val_x_batch, val_y_batch in train_loader: \n",
    "        \n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Initialise hidden state\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        \n",
    "        # Training: Predict and calc loss\n",
    "        train_pred = model(x_batch)\n",
    "        # Flatten to (batch_size*timesteps) x 4\n",
    "        train_pred = train_pred.view(train_pred.shape[0]*train_pred.shape[1], 4) \n",
    "        all_train_pred.append(train_pred)\n",
    "        loss = zoib.zoib_loss(\n",
    "            train_pred,\n",
    "            y_batch.view(y_batch.shape[0]*y_batch.shape[1], 1)[:,0] # Flatten to (batch_size*timesteps) X 1\n",
    "        ).float()\n",
    "        \n",
    "        # Tracking mean loss across batches\n",
    "        epoch_loss += train_pred.shape[0]*loss.item()\n",
    "        total_items += train_pred.shape[0]\n",
    "        \n",
    "        \n",
    "        # Validation: predict and calc loss\n",
    "        val_pred = model(val_x_batch)\n",
    "        # Flatten to (batch_size*timesteps) x 4\n",
    "        val_pred = val_pred.view(val_pred.shape[0]*val_pred.shape[1], 4)\n",
    "        all_val_pred.append(val_pred)\n",
    "        val_loss = zoib.zoib_loss(\n",
    "            val_pred, \n",
    "            val_y_batch.view(val_y_batch.shape[0]*val_y_batch.shape[1], 1)[:,0]  # Flatten to (batch_size*timesteps) X 1\n",
    "        ).float()\n",
    "\n",
    "        # Tracking mean loss across batches\n",
    "        val_epoch_loss += val_pred.shape[0]*val_loss.item()\n",
    "        val_total_items += val_pred.shape[0]\n",
    "\n",
    "        \n",
    "        # Zero out gradient\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "\n",
    "    # LR decay\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = epoch_loss/total_items\n",
    "    val_epoch_loss = val_epoch_loss/val_total_items\n",
    "    loss_history.append(epoch_loss)\n",
    "    val_loss_history.append(val_epoch_loss)\n",
    "    if t%10==0:\n",
    "        print(\"Epoch \", t, \"Train Loss: \", epoch_loss, \", Val Loss: \", val_epoch_loss, \", LR: \", optimiser.param_groups[0][\"lr\"])\n",
    "        \n",
    "    if np.isnan(epoch_loss):\n",
    "        break\n",
    "        \n",
    "    # Early stopping\n",
    "    if epoch_loss < best_loss:\n",
    "        i = 0\n",
    "        # Save best loss, predictoins, and hidden state\n",
    "        best_loss = epoch_loss\n",
    "        best_train_pred = torch.cat(all_train_pred, dim=0)\n",
    "        best_val_pred = torch.cat(all_val_pred, dim=0)\n",
    "        best_hidden_1, best_hidden_2 = model.hidden[0].clone().detach().numpy(), model.hidden[1].clone().detach().numpy()\n",
    "    \n",
    "    if (i > early_stopping):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoib_expected(t):\n",
    "    # E = q*(1-p) + (1-p-q)*(conc1/(conc1+conc0))\n",
    "    # Or # = prob_1_given_not0*(1-prob_0) + (1 - prob_bernoulli)*(expect_val_beta)\n",
    "    t = t.detach().numpy()\n",
    "    prob_1 = t[:,1]*(1-t[:,0])\n",
    "    prob_beta = (1 - t[:,0])*(1 - t[:,1])\n",
    "    beta_expected = t[:,2]/(t[:,3]+t[:,2])\n",
    "    return prob_1 + prob_beta*beta_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(zoib_expected(best_train_pred), train_y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':zoib_expected(best_train_pred), 'True':train_y.flatten()}).plot(xlim=[0,120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(zoib_expected(best_val_pred), val_y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':zoib_expected(best_val_pred), 'True':val_y.flatten()}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.DataFrame(best_train_pred.detach().numpy())\n",
    "param_df.columns = ['p','q','conc1','conc0']\n",
    "pd.plotting.scatter_matrix(param_df)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playa_venv",
   "language": "python",
   "name": "playa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
