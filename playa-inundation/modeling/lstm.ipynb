{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Robert Guthrie https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "# And: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import json\n",
    "import zoib\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(df):\n",
    "    ind_year = np.where(np.array(traj.index.names)=='year')[0][0]\n",
    "    train_df = df.loc[df.index.get_level_values(ind_year)<=2010]\n",
    "    val_df = df.loc[(df.index.get_level_values(ind_year)>2010) & (df.index.get_level_values(ind_year)<=2014)]\n",
    "    test_df = df.loc[df.index.get_level_values(ind_year)>2014]\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_lc_frac_df(ids=[]):\n",
    "    \"\"\"LC Frac csv is hardcoded! Change if you need it\"\"\"\n",
    "    lc_df = pd.read_csv('../data/fraster_landcover_allyears_bigger.csv').set_index('id')\n",
    "    if len(ids)>0:\n",
    "        lc_df = lc_df.loc[ids]\n",
    "    lc_frac = pd.DataFrame()\n",
    "    for col in lc_df.columns:\n",
    "        year = int(col[0:4])\n",
    "        jsond = lc_df[col].str.replace(r'([0-9]+)(:)', r'\"\\1\"\\2', regex=True).apply(json.loads)\n",
    "        temp_frac_df = (pd.json_normalize(jsond)/5000)\n",
    "        temp_frac_df.columns = ['lcf{}'.format(lc) for lc in temp_frac_df.columns]\n",
    "        temp_frac_df = temp_frac_df.assign(id=lc_df.index, year=year)\n",
    "        lc_frac = lc_frac.append(temp_frac_df)\n",
    "    lc_frac.fillna(0,inplace=True)\n",
    "    \n",
    "    return lc_frac.set_index(['id','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_join_csv(inun_csv, drop_zeros=True):\n",
    "    # Prep inundation data\n",
    "    inun_df = pd.read_csv(inun_csv)\n",
    "    inun_df.set_index(['id','year','month'], inplace=True)\n",
    "    inun_df = inun_df.loc[~inun_df['inundation'].isna()]\n",
    "    if drop_zeros:\n",
    "        max_inun = inun_df.groupby('id').agg({'inundation':'max'})\n",
    "        zero_ids = max_inun.loc[max_inun['inundation']==0].index\n",
    "        inun_df.drop(zero_ids, inplace=True)\n",
    "        if inun_df.shape[0]==0:\n",
    "            return \n",
    "        \n",
    "    # Prep weather data\n",
    "    weather_csv = inun_csv.replace('inun_frac_','weather_')\n",
    "    weather_df = pd.read_csv(weather_csv)\n",
    "    weather_df.set_index(['id','year','month'], inplace=True)\n",
    "    joined_df = weather_df.join(inun_df, how='inner')\n",
    "    \n",
    "    # Finally, prep landcover fraction dataframe\n",
    "    # Both prep and join are a bit slow\n",
    "    # Could prep into fractions ahead of time\n",
    "    # And also split up lc df by county\n",
    "    lc_frac_df = prep_lc_frac_df(ids=joined_df.index.get_level_values(0).unique())\n",
    "    joined_df = joined_df.join(lc_frac_df, how='inner')\n",
    "    \n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_num_playas = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inun_csv_list = glob.glob('../data/state_county_csvs/counties/inun_frac*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.DataFrame()\n",
    "while joined_df.index.get_level_values(0).unique().shape[0] <= target_num_playas:\n",
    "    rand_csv = np.random.choice(inun_csv_list)\n",
    "    joined_df = pd.concat([joined_df, read_join_csv(rand_csv, drop_zeros=True)])\n",
    "    \n",
    "joined_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    # Adapted from: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    old_cols = data.columns\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.groupby('id').shift(i))\n",
    "        names += [('%s(t-%d)' % (old_cols[j], i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('%s(t)' % (old_cols[j])) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('%s(t+%d)' % (old_cols[j], i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = joined_df.loc[joined_df.index.get_level_values(0)]\n",
    "traj = traj.drop(columns=['area'])#[['inundation', 'acres', 'vpd', 'temp','precip']]\n",
    "n_features = traj.shape[1]\n",
    "traj['inundation'].plot()\n",
    "print(traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = series_to_supervised(traj, n_in=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_timeseries = traj.loc[traj.index.get_level_values(0)[0]].shape[0]\n",
    "new_ids = np.array([\n",
    "    np.repeat(i, len_of_timeseries) for i in range(traj.index.get_level_values(0).unique().shape[0])]\n",
    ").flatten()\n",
    "# traj = traj.assign(id=traj.index.get_level_values(0)) # Put id at end for embedding\n",
    "traj = traj.assign(id=new_ids) # Put id at end for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop inundation to end\n",
    "inun = traj.pop('inundation(t)')\n",
    "traj['inundation(t)'] = inun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorfy(x, y, batch_size):\n",
    "    batch_starts = np.arange(0, x.shape[0], batch_size)\n",
    "    x_tensor = [torch.tensor(np.array(x[i:(i+batch_size)])).float() for i in batch_starts]\n",
    "    if len(x_tensor[-1]) < batch_size: # drop last batch if not even\n",
    "        y = y[:-len(x_tensor[-1])]\n",
    "        x_tensor = x_tensor[:-1]\n",
    "    return x_tensor, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train, val, test = split_train_test_val(traj)\n",
    "train_X, train_y = train.values[:, :-1], train.values[:, -1]\n",
    "val_X, val_y = val.values[:, :-1], val.values[:, -1]\n",
    "test_X, test_y = test.values[:, :-1], test.values[:, -1]\n",
    "# Run scaler, but not on ID\n",
    "train_X[:,:-1] = scaler.fit_transform(train_X[:,:-1])\n",
    "val_X[:,:-1] = scaler.transform(val_X[:,:-1])\n",
    "test_X[:,:-1] = scaler.transform(test_X[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24# int(train_X.shape[0]/traj.index.get_level_values(0).unique().shape[0])\n",
    "batch_size_val = 48#int(val_X.shape[0]/traj.index.get_level_values(0).unique().shape[0])\n",
    "num_playas = int(train_X[:, -1].max())+1\n",
    "lstm_input_size = traj.shape[1]-1\n",
    "\n",
    "# Params to set\n",
    "hidden_dim = 128\n",
    "embedding_dim = 2\n",
    "loss_fn = 'zoib' # 'mae' or 'zoib'\n",
    "if loss_fn=='zoib':\n",
    "    output_dim=4\n",
    "else:\n",
    "    output_dim=1\n",
    "num_layers=1\n",
    "learning_rate = 0.1\n",
    "num_epochs = 5000\n",
    "regularization_weight = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, val, and test sets\n",
    "train_X_tensor, train_y = tensorfy(train_X, train_y, batch_size)\n",
    "val_X_tensor, val_y = tensorfy(val_X, val_y, batch_size_val)\n",
    "test_X_tensor, test_y = tensorfy(test_X, test_y, batch_size_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, num_playas, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_playas = num_playas\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.embedding = nn.Embedding(self.num_playas, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(self.input_dim + self.embedding_dim - 1, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.exp = torch.exp\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def relu_01(self, x):\n",
    "        x = torch.max(torch.zeros_like(x), torch.min(torch.ones_like(x), x))\n",
    "        return x\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        input_reshape = torch.cat(input).view(len(input), self.batch_size, -1)\n",
    "        self.emb_layer = self.embedding(input_reshape[:,:,-1].long())\n",
    "        \n",
    "        # Concat and run through LSTM\n",
    "        lstm_out, self.hidden = self.lstm(torch.cat((input_reshape[:,:,:-1],self.emb_layer), 2))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        lin_act = self.linear(lstm_out)\n",
    "#         y_pred = self.sigmoid(lin_act)\n",
    "#         y_pred = torch.cat((self.sigmoid(lin_act[:,:,0:2]),self.relu(lin_act[:,:,2:4])), 2)\n",
    "        y_pred = torch.cat((self.sigmoid(lin_act[:,:,0:2]),torch.exp(lin_act[:,:,2:4])), 2)\n",
    "\n",
    "    \n",
    "        return y_pred#(y_pred-y_pred.min())/(y_pred.max()-y_pred.min())\n",
    "\n",
    "\n",
    "model = LSTM(input_dim = lstm_input_size,\n",
    "             embedding_dim=embedding_dim,\n",
    "             num_playas=num_playas,\n",
    "             hidden_dim=hidden_dim,\n",
    "             batch_size=batch_size,\n",
    "             output_dim=output_dim,\n",
    "             num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = nn.L1Loss()\n",
    "    \n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=regularization_weight)\n",
    "#####################---------------------------------------------------------------------------\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    # Clear stored gradient\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    # Forward pass\n",
    "    model.batch_size=batch_size\n",
    "    train_pred = model(train_X_tensor) #.requires_grad_(True)\n",
    "    \n",
    "    # Val pred\n",
    "    model.batch_size=batch_size_val\n",
    "    val_pred = model(val_X_tensor)\n",
    "    \n",
    "    if loss_fn=='zoib':\n",
    "        train_pred = train_pred.view(train_pred.shape[0]*train_pred.shape[1], 4)\n",
    "        loss = zoib.zoib_loss(train_pred, train_y).float()\n",
    "        val_pred = val_pred.view(val_pred.shape[0]*val_pred.shape[1], 4)\n",
    "        val_loss = zoib.zoib_loss(val_pred, val_y).float()\n",
    "    else:\n",
    "        train_pred = train_pred.view(train_pred.shape[0]*train_pred.shape[1])\n",
    "        loss = l1_loss(train_pred, torch.tensor(train_y)).float()\n",
    "        val_pred = val_pred.view(val_pred.shape[0]*val_pred.shape[1])\n",
    "        val_loss = l1_loss(val_pred, torch.tensor(val_y)).float()\n",
    "    \n",
    "\n",
    "\n",
    "    if t%50==0:\n",
    "        print(\"Epoch \", t, \"Train Loss: \", loss.item(), \", Val Loss: \", val_loss.item())\n",
    "    hist[t] = loss\n",
    "    if torch.isnan(loss).item():\n",
    "        break\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser.step()\n",
    "    \n",
    "    last_train_pred = train_pred.clone()\n",
    "    last_val_pred = val_pred.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoib_expected(t):\n",
    "    # E = q*(1-p) + (1-p-q)*(conc1/(conc1+conc0))\n",
    "    # Or # = prob_1_given_not0*(1-prob_0) + (1 - prob_bernoulli)*(expect_val_beta)\n",
    "    t = t.detach().numpy()\n",
    "    prob_1 = t[:,1]*(1-t[:,0])\n",
    "#     prob_bern = t[:,0]+t[:,1]\n",
    "    prob_beta = (1 - t[:,0])*(1 - t[:,1])\n",
    "    beta_expected = t[:,2]/(t[:,3]+t[:,2])\n",
    "    return prob_1 + prob_beta*beta_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(zoib_expected(last_train_pred), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':zoib_expected(last_train_pred), 'True':train_y}).plot(xlim=[0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(zoib_expected(last_val_pred), val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':zoib_expected(last_val_pred), 'True':val_y}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.DataFrame(last_train_pred.detach().numpy())\n",
    "param_df.columns = ['p','q','conc1','conc0']\n",
    "pd.plotting.scatter_matrix(param_df)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playa_venv",
   "language": "python",
   "name": "playa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
