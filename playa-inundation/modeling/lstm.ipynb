{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Robert Guthrie\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(df):\n",
    "    ind_year = np.where(np.array(traj.index.names)=='year')[0][0]\n",
    "    train_df = df.loc[df.index.get_level_values(ind_year)<=2010]\n",
    "    val_df = df.loc[(df.index.get_level_values(ind_year)>2010) & (df.index.get_level_values(ind_year)<=2014)]\n",
    "    test_df = df.loc[df.index.get_level_values(ind_year)>2014]\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inun_csv_list = glob.glob('../data/state_county_csvs/counties/inun_frac*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_csv = np.random.choice(inun_csv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = read_join_csv(rand_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    old_cols = data.columns\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('%s(t-%d)' % (old_cols[j], i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('%s(t)' % (old_cols[j])) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('%s(t+%d)' % (old_cols[j], i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = joined_df.loc[joined_df.index.get_level_values(0)[0]]\n",
    "traj = traj[['inundation', 'acres', 'vpd', 'temp','precip']]\n",
    "n_features = traj.shape[1]\n",
    "traj['inundation'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = series_to_supervised(traj)\n",
    "traj = traj.drop(columns=traj.columns[range(-(n_features-1),0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set params, start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split_train_test_val(traj)\n",
    "train = scaler.fit_transform(train)\n",
    "val = scaler.transform(val)\n",
    "test = scaler.transform(test)\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "val_X, val_y = val[:, :-1], val[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "batch_size_val = 8\n",
    "lstm_input_size = traj.shape[1]-1\n",
    "hidden_dim = 50\n",
    "output_dim=1\n",
    "num_layers=1\n",
    "learning_rate = 0.01\n",
    "num_epochs =200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorfy(x, y, batch_size):\n",
    "    batch_starts = np.arange(0, x.shape[0], batch_size)\n",
    "    x_tensor = [torch.tensor(np.array(x[i:(i+batch_size)])).float() for i in batch_starts]\n",
    "    if len(x_tensor[-1]) < batch_size: # drop last batch if not even\n",
    "        y = y[:-len(x_tensor[-1])]\n",
    "        x_tensor = x_tensor[:-1]\n",
    "    return x_tensor, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor, train_y = tensorfy(train_X, train_y, batch_size)\n",
    "val_X_tensor, val_y = tensorfy(val_X, val_y, batch_size_val)\n",
    "test_X_tensor, test_y = tensorfy(test_X, test_y, batch_size_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(torch.cat(input).view(len(input), self.batch_size, -1))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out)\n",
    "        return y_pred.view(-1)\n",
    "\n",
    "\n",
    "model = LSTM(input_dim = lstm_input_size,\n",
    "             hidden_dim=50,\n",
    "             batch_size=batch_size,\n",
    "             output_dim=1,\n",
    "             num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    # Clear stored gradient\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    # Forward pass\n",
    "    model.batch_size=batch_size\n",
    "    train_pred = model(train_X_tensor)\n",
    "\n",
    "    loss = loss_fn(train_pred, torch.tensor(train_y).float())\n",
    "    \n",
    "    model.batch_size=batch_size_val\n",
    "    val_pred = model(val_X_tensor)\n",
    "    val_loss = loss_fn(val_pred, torch.tensor(val_y).float())\n",
    "    print(\"Epoch \", t, \"Train MAE: \", loss.item(), \", Val MAE: \", val_loss.item())\n",
    "    hist[t] = loss.item()\n",
    "\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_pred.detach().numpy(), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':train_pred.detach().numpy(), 'True':train_y}).plot(xlim=(200,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model(val_X_tensor).detach().numpy(), val_y)\n",
    "pd.DataFrame({'Pred':model(val_X_tensor).detach().numpy(), 'True':val_y}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model(test_X_tensor).detach().numpy(), test_y)\n",
    "pd.DataFrame({'Pred':model(test_X_tensor).detach().numpy(), 'True':test_y}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playa_venv",
   "language": "python",
   "name": "playa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
