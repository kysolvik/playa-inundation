{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Robert Guthrie https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "# And: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(df):\n",
    "    ind_year = np.where(np.array(traj.index.names)=='year')[0][0]\n",
    "    train_df = df.loc[df.index.get_level_values(ind_year)<=2010]\n",
    "    val_df = df.loc[(df.index.get_level_values(ind_year)>2010) & (df.index.get_level_values(ind_year)<=2014)]\n",
    "    test_df = df.loc[df.index.get_level_values(ind_year)>2014]\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_join_csv(inun_csv, drop_zeros=True):\n",
    "    # Prep inundation data\n",
    "    inun_df = pd.read_csv(inun_csv)\n",
    "    inun_df.set_index(['id','year','month'], inplace=True)\n",
    "    inun_df = inun_df.loc[~inun_df['inundation'].isna()]\n",
    "    if drop_zeros:\n",
    "        max_inun = inun_df.groupby('id').agg({'inundation':'max'})\n",
    "        zero_ids = max_inun.loc[max_inun['inundation']==0].index\n",
    "        inun_df.drop(zero_ids, inplace=True)\n",
    "        if inun_df.shape[0]==0:\n",
    "            return \n",
    "        \n",
    "    # Prep weather data\n",
    "    weather_csv = inun_csv.replace('inun_frac_','weather_')\n",
    "    weather_df = pd.read_csv(weather_csv)\n",
    "    weather_df.set_index(['id','year','month'], inplace=True)\n",
    "    joined_df = weather_df.join(inun_df, how='inner')\n",
    "    \n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inun_csv_list = glob.glob('../data/state_county_csvs/counties/inun_frac*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_csv = np.random.choice(inun_csv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = read_join_csv(rand_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    old_cols = data.columns\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.groupby('id').shift(i))\n",
    "        names += [('%s(t-%d)' % (old_cols[j], i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('%s(t)' % (old_cols[j])) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('%s(t+%d)' % (old_cols[j], i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = joined_df#.loc[joined_df.index.get_level_values(0)[0]]\n",
    "traj = traj[['inundation', 'acres', 'vpd', 'temp','precip']]\n",
    "n_features = traj.shape[1]\n",
    "traj['inundation'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = series_to_supervised(traj)\n",
    "# If you want to remove current weather vars\n",
    "#traj = traj.drop(columns=traj.columns[range(-(n_features-1),0)])\n",
    "# If you want to remove last inundation from model\n",
    "# traj = traj.drop(columns=['inundation(t-1)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding (if multiple series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_embed = nn.Embedding(70000, 5) # Simple for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings\n",
    "embeds_id = id_embed(torch.tensor(traj.index.get_level_values(0))).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = pd.concat([traj.reset_index(), pd.DataFrame(embeds_id, columns=['id{}'.format(i) for i in range(embeds_id.shape[1])])],\n",
    "          axis=1)\n",
    "traj.set_index(['id','year','month'], inplace=True)\n",
    "# Pop inundation to end\n",
    "inun = traj.pop('inundation(t)')\n",
    "traj['inundation(t)'] = inun\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorfy(x, y, batch_size):\n",
    "    batch_starts = np.arange(0, x.shape[0], batch_size)\n",
    "    x_tensor = [torch.tensor(np.array(x[i:(i+batch_size)])).float() for i in batch_starts]\n",
    "    if len(x_tensor[-1]) < batch_size: # drop last batch if not even\n",
    "        y = y[:-len(x_tensor[-1])]\n",
    "        x_tensor = x_tensor[:-1]\n",
    "    return x_tensor, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train, val, test = split_train_test_val(traj)\n",
    "train = scaler.fit_transform(train)\n",
    "val = scaler.transform(val)\n",
    "test = scaler.transform(test)\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "val_X, val_y = val[:, :-1], val[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(train_X.shape[0]/traj.index.get_level_values(0).unique().shape[0])\n",
    "batch_size_val = int(val_X.shape[0]/traj.index.get_level_values(0).unique().shape[0])\n",
    "lstm_input_size = traj.shape[1]-1\n",
    "hidden_dim = 50\n",
    "loss_fn = 'mae' # 'mae' or 'zoib'\n",
    "if loss_fn=='zoib':\n",
    "    output_dim=4\n",
    "else:\n",
    "    output_dim=1\n",
    "num_layers=1\n",
    "learning_rate = 0.01\n",
    "num_epochs =1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor, train_y = tensorfy(train_X, train_y, batch_size)\n",
    "val_X_tensor, val_y = tensorfy(val_X, val_y, batch_size_val)\n",
    "test_X_tensor, test_y = tensorfy(test_X, test_y, batch_size_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def relu_01(self, x):\n",
    "        x = torch.max(torch.zeros_like(x), torch.min(torch.ones_like(x), x))\n",
    "        return x\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.randn(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.randn(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(torch.cat(input).view(len(input), self.batch_size, -1))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out)\n",
    "\n",
    "        return y_pred#(y_pred-y_pred.min())/(y_pred.max()-y_pred.min())\n",
    "\n",
    "\n",
    "model = LSTM(input_dim = lstm_input_size,\n",
    "             hidden_dim=hidden_dim,\n",
    "             batch_size=batch_size,\n",
    "             output_dim=output_dim,\n",
    "             num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zoib\n",
    "\n",
    "def zoib_loss(t, y_true, pad=0.0001):\n",
    "    log_probs = zoib.ZOIBeta(\n",
    "        p=t[:,0]+pad,\n",
    "        q=t[:,1]+pad,\n",
    "        concentration1=t[:,2]+pad,\n",
    "        concentration0=t[:,3]+pad\n",
    "    ).log_prob(torch.tensor(y_true).float())\n",
    "    \n",
    "    return torch.mean(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = nn.L1Loss()\n",
    "    \n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    # Clear stored gradient\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    # Forward pass\n",
    "    model.batch_size=batch_size\n",
    "    train_pred = model(train_X_tensor) #.requires_grad_(True)\n",
    "    \n",
    "    # Val pred\n",
    "    model.batch_size=batch_size_val\n",
    "    val_pred = model(val_X_tensor)\n",
    "    \n",
    "    if loss_fn=='zoib':\n",
    "        train_pred = train_pred.view(train_pred.shape[0]*train_pred.shape[1], 4)\n",
    "        loss = zoib_loss(train_pred, train_y).float()\n",
    "        val_pred = val_pred.view(val_pred.shape[0]*val_pred.shape[1], 4)\n",
    "        val_loss = zoib_loss(val_pred, val_y).float()\n",
    "    else:\n",
    "        train_pred = train_pred.view(train_pred.shape[0]*train_pred.shape[1])\n",
    "        loss = l1_loss(train_pred, torch.tensor(train_y)).float()\n",
    "        val_pred = val_pred.view(val_pred.shape[0]*val_pred.shape[1])\n",
    "        val_loss = l1_loss(val_pred, torch.tensor(val_y)).float()\n",
    "    \n",
    "\n",
    "\n",
    "    if t%50==0:\n",
    "        print(\"Epoch \", t, \"Train Loss: \", loss.item(), \", Val Loss: \", val_loss.item())\n",
    "    hist[t] = loss\n",
    "    if torch.isnan(loss).item():\n",
    "        break\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser.step()\n",
    "    \n",
    "    last_pred=train_pred.detach().numpy().copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_pred.detach().numpy(), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':train_pred.detach().numpy(), 'True':train_y}).plot(xlim=[1500,1550])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(val_pred.detach().numpy(), val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':val_pred.detach().numpy(), 'True':val_y}).plot(xlim=[350,400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Zoib\n",
    "## zoib_mean is definitely not the right formula for expected value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoib_mean(all_vals):\n",
    "     return all_vals[:,3] + all_vals[:,0]/(all_vals[:,0]+all_vals[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoib_loss(torch.tensor(last_pred), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(zoib_mean(last_pred), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':zoib_mean(last_pred), 'True':train_y}).plot(xlim=(0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model(val_X_tensor).detach().numpy(), val_y)\n",
    "pd.DataFrame({'Pred':model(val_X_tensor).detach().numpy(), 'True':val_y}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model(test_X_tensor).detach().numpy(), test_y)\n",
    "pd.DataFrame({'Pred':model(test_X_tensor).detach().numpy(), 'True':test_y}).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playa_venv",
   "language": "python",
   "name": "playa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
