{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Robert Guthrie https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "# And: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import json\n",
    "import zoib\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(df):\n",
    "    ind_year = np.where(np.array(df.index.names)=='year')[0][0]\n",
    "    train_df = df.loc[df.index.get_level_values(ind_year)<=2010]\n",
    "    val_df = df.loc[(df.index.get_level_values(ind_year)>2010) & (df.index.get_level_values(ind_year)<=2014)]\n",
    "    test_df = df.loc[df.index.get_level_values(ind_year)>2014]\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of playas from 1 to 8\n",
    "num_playas = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv and set to appropriate number of playas\n",
    "traj = pd.read_csv('./prepped_8.csv')\n",
    "traj = traj.loc[traj['new_id'].isin(range(num_playas))]\n",
    "# Set indices\n",
    "traj.set_index(['year','month','id'], inplace=True)\n",
    "\n",
    "# Drop embedding new_id\n",
    "# traj.drop(columns=['new_id'], inplace=True)\n",
    "\n",
    "# Here's a chance to drop everything else, if you'd like\n",
    "traj = traj[['precip', 'new_id','inundation']]\n",
    "traj['precip'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params to set\n",
    "hidden_dim = 64\n",
    "\n",
    "num_layers=1\n",
    "learning_rate = 0.005\n",
    "num_epochs = 100\n",
    "weight_decay = 0\n",
    "lr_gamma = 1.0\n",
    "lr_decay_step_size = 10 # Set this high if not needed\n",
    "regularization_weight = 0\n",
    "batch_size = 2\n",
    "\n",
    "early_stopping=100000 # Set high if not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_id(x, y):\n",
    "    seq_length = np.sum((x[:,-1]==x[0,-1]))\n",
    "    seq_starts = np.arange(0, x.shape[0], seq_length)\n",
    "    x_arr = np.array([np.array(x[i:(i+seq_length)]) for i in seq_starts])\n",
    "    y_arr = np.array([np.array(y[i:(i+seq_length)]) for i in seq_starts])\n",
    "    return x_arr, y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train, val, test = split_train_test_val(traj)\n",
    "train_X, train_y = train.values[:, :-1], train.values[:, -1]\n",
    "val_X, val_y = val.values[:, :-1], val.values[:, -1]\n",
    "test_X, test_y = test.values[:, :-1], test.values[:, -1]\n",
    "# Run scaler, but not on ID\n",
    "# train_X[:,:-1] = scaler.fit_transform(train_X[:,:-1])\n",
    "# val_X[:,:-1] = scaler.transform(val_X[:,:-1])\n",
    "# test_X[:,:-1] = scaler.transform(test_X[:,:-1])\n",
    "\n",
    "num_playas = int(train_X[:, -1].max())+1\n",
    "lstm_input_size = traj.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into arrays by id\n",
    "train_X_array, train_y = split_by_id(train_X, train_y)\n",
    "val_X_array, val_y = split_by_id(val_X, val_y)\n",
    "test_X_array, test_y = split_by_id(test_X, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torch.utils.data.TensorDataset(torch.Tensor(train_X_array), torch.Tensor(train_y))\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "val_ds = torch.utils.data.TensorDataset(torch.Tensor(val_X_array), torch.Tensor(val_y))\n",
    "val_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1,\n",
    "                    num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_playas = num_playas\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "\n",
    "        # Define activations for output\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "    \n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of input: [batch_size, timesteps, input_dims]\n",
    "        # shape of lstm_out: [batch_size, timesteps, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        # Shape of y_pred: [batch_size, timesteps, 1]\n",
    "        # Run ids through embedding layer        \n",
    "        # Concat and run through LSTM\n",
    "        # Initialize hidden and cell state\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_dim)\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, input.size(0), self.hidden_dim)\n",
    "        \n",
    "        assert h0.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "        assert c0.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "\n",
    "        \n",
    "        lstm_out, (hn, cn) = self.lstm(input, (h0, c0))\n",
    "        \n",
    "        # Assert that shapes are still as expected\n",
    "        assert hn.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "        assert cn.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "        assert lstm_out.shape == torch.Size([input.size(0), input.size(1), self.hidden_dim]) # batch, seq_len, hidden\n",
    "\n",
    "        \n",
    "        # Run activation and get outputs\n",
    "        lin_act = self.linear(lstm_out)\n",
    "        # Expect [batch_size, seq_len, 1]\n",
    "        assert lin_act.shape == torch.Size([input.size(0), input.size(1), 1])\n",
    "        return lin_act\n",
    "\n",
    "\n",
    "model = LSTM(input_dim = lstm_input_size,\n",
    "             hidden_dim=hidden_dim,\n",
    "             output_dim=1,\n",
    "             num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = nn.L1Loss()\n",
    "    \n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=lr_decay_step_size, gamma=lr_gamma)\n",
    "#####################---------------------------------------------------------------------------\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "best_loss = 1000\n",
    "for t in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    val_epoch_loss = 0\n",
    "    total_items = 0\n",
    "    val_total_items = 0\n",
    "    # List to store all predictions\n",
    "    all_train_pred = []\n",
    "    all_val_pred = []\n",
    "    for x_batch, y_batch in train_loader: \n",
    "        \n",
    "        # Clear stored gradient\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Training: Predict and calc loss\n",
    "        train_pred = model(x_batch)\n",
    "        loss = l1_loss(train_pred.view(x_batch.size(0), -1), y_batch)\n",
    "        all_train_pred.append(train_pred)\n",
    "        \n",
    "        # Tracking mean loss across batches\n",
    "        epoch_loss += train_pred.shape[0]*loss.item()\n",
    "        total_items += train_pred.shape[0]\n",
    "        \n",
    "        \n",
    "#         # Validation: predict and calc loss\n",
    "#         val_pred = model(val_x_batch)\n",
    "#         # Flatten to (batch_size*timesteps) x 4\n",
    "#         val_pred = val_pred.view(val_pred.shape[0]*val_pred.shape[1], 4)\n",
    "#         all_val_pred.append(val_pred)\n",
    "#         val_loss = zoib.zoib_loss(\n",
    "#             val_pred, \n",
    "#             val_y_batch.view(val_y_batch.shape[0]*val_y_batch.shape[1], 1)[:,0]  # Flatten to (batch_size*timesteps) X 1\n",
    "#         ).float()\n",
    "\n",
    "#         # Tracking mean loss across batches\n",
    "#         val_epoch_loss += val_pred.shape[0]*val_loss.item()\n",
    "#         val_total_items += val_pred.shape[0]\n",
    "\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "\n",
    "    # LR decay\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = epoch_loss/total_items\n",
    "#     val_epoch_loss = val_epoch_loss/val_total_items\n",
    "    loss_history.append(epoch_loss)\n",
    "#     val_loss_history.append(val_epoch_loss)\n",
    "    if t%10==0:\n",
    "        print(\"Epoch \", t, \"Train Loss: \", epoch_loss, \"LR: \", optimiser.param_groups[0][\"lr\"])\n",
    "        \n",
    "    if np.isnan(epoch_loss):\n",
    "        break\n",
    "        \n",
    "    # Early stopping\n",
    "    if epoch_loss < best_loss:\n",
    "        i = 0\n",
    "        # Save best loss, predictoins, and hidden state\n",
    "        best_loss = epoch_loss\n",
    "        best_train_pred = torch.cat(all_train_pred, dim=0)\n",
    "#         best_val_pred = torch.cat(all_val_pred, dim=0)    \n",
    "    if (i > early_stopping):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_y.flatten(), best_train_pred.detach().numpy().flatten())\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':best_train_pred.detach().numpy().flatten(), 'True':train_y.flatten()}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playa_venv",
   "language": "python",
   "name": "playa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
