{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Robert Guthrie https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "# And: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import json\n",
    "import zoib\n",
    "\n",
    "np.random.seed(55)\n",
    "torch.manual_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(df):\n",
    "    ind_year = np.where(np.array(df.index.names)=='year')[0][0]\n",
    "    train_df = df.loc[df.index.get_level_values(ind_year)<=2010]\n",
    "    val_df = df.loc[(df.index.get_level_values(ind_year)>2010) & (df.index.get_level_values(ind_year)<=2014)]\n",
    "    test_df = df.loc[df.index.get_level_values(ind_year)>2014]\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of playas from 1 to 8\n",
    "num_playas = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv and set to appropriate number of playas\n",
    "traj = pd.read_csv('./prepped_8.csv')\n",
    "traj = traj.loc[traj['new_id'].isin(range(num_playas))]\n",
    "# Set indices\n",
    "traj.set_index(['year','month','id'], inplace=True)\n",
    "\n",
    "# Drop embedding new_id\n",
    "# traj.drop(columns=['new_id'], inplace=True)\n",
    "\n",
    "# Here's a chance to drop everything else, if you'd like\n",
    "traj = traj[['precip', 'temp','vpd','new_id','inundation']]\n",
    "traj['inundation'] = np.arange(traj.shape[0]).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params to set\n",
    "hidden_dim = 32\n",
    "num_layers=1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "weight_decay = 0\n",
    "lr_gamma = 1.0\n",
    "lr_decay_step_size = 10 # Set this high if not needed\n",
    "regularization_weight = 0\n",
    "batch_size = 8\n",
    "\n",
    "early_stopping=100000 # Set high if not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_id(x, y, seq_length):\n",
    "    seq_starts = np.arange(0, x.shape[0], seq_length)\n",
    "    x_arr = np.array([np.array(x[i:(i+seq_length)]) for i in seq_starts])\n",
    "    y_arr = np.array([np.array(y[i:(i+seq_length)]) for i in seq_starts])\n",
    "    return x_arr, y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split_train_test_val(traj)\n",
    "train_X, train_y = train.values[:, :-1], train.values[:, -1]\n",
    "val_X, val_y = val.values[:, :-1], val.values[:, -1]\n",
    "test_X, test_y = test.values[:, :-1], test.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train, val, test = split_train_test_val(traj)\n",
    "train_X, train_y = train.values[:, :-1], train.values[:, -1]\n",
    "val_X, val_y = val.values[:, :-1], val.values[:, -1]\n",
    "test_X, test_y = test.values[:, :-1], test.values[:, -1]\n",
    "# Run scaler, but not on ID\n",
    "# train_X[:,:] = scaler.fit_transform(train_X[:,:])\n",
    "# val_X[:,:] = scaler.transform(val_X[:,:])\n",
    "# test_X[:,:] = scaler.transform(test_X[:,:])\n",
    "\n",
    "# num_playas = int(train_X[:, -1].max())+1\n",
    "lstm_input_size = traj.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into arrays by id\n",
    "train_X_array, train_y = split_by_id(train_X, train_y, seq_length=int(train.shape[0]/num_playas))\n",
    "val_X_array, val_y = split_by_id(val_X, val_y, seq_length=int(val.shape[0]/num_playas))\n",
    "test_X_array, test_y = split_by_id(test_X, test_y, seq_length=int(test.shape[0]/num_playas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_ds = torch.utils.data.TensorDataset(torch.Tensor(train_X_array), torch.Tensor(train_y), torch.Tensor(val_X_array), torch.Tensor(val_y))\n",
    "train_val_loader = torch.utils.data.DataLoader(train_val_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1,\n",
    "                    num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_playas = num_playas\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "\n",
    "        # Define activations for output\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.exp = torch.exp\n",
    "    \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        self.h = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        self.c = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        return\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of input: [batch_size, timesteps, input_dims]\n",
    "        # shape of lstm_out: [batch_size, timesteps, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        # Shape of y_pred: [batch_size, timesteps, 1]\n",
    "        # Run ids through embedding layer        \n",
    "        # Concat and run through LSTM\n",
    "        \n",
    "        assert self.h.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "        assert self.c.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "\n",
    "        \n",
    "        lstm_out, (self.h, self.c) = self.lstm(input, (self.h, self.c))\n",
    "        \n",
    "        # Assert that shapes are still as expected\n",
    "        assert self.h.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "        assert self.c.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "        assert lstm_out.shape == torch.Size([input.size(0), input.size(1), self.hidden_dim]) # batch, seq_len, hidden\n",
    "\n",
    "        \n",
    "        # Run activation and get outputs\n",
    "        lin_act = self.linear(lstm_out)\n",
    "        exp_act = self.exp(lin_act)\n",
    "        # Expect [batch_size, seq_len, 1]\n",
    "        assert lin_act.shape == torch.Size([input.size(0), input.size(1), 1])\n",
    "        assert lin_act.shape == exp_act.shape\n",
    "\n",
    "        return exp_act\n",
    "\n",
    "\n",
    "model = LSTM(input_dim = lstm_input_size,\n",
    "             hidden_dim=hidden_dim,\n",
    "             output_dim=1,\n",
    "             num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = nn.L1Loss()\n",
    "    \n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=lr_decay_step_size, gamma=lr_gamma)\n",
    "#####################---------------------------------------------------------------------------\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "best_loss = 1000\n",
    "train_y_ordered = []\n",
    "val_y_ordered = []\n",
    "for t in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    val_epoch_loss = 0\n",
    "    total_items = 0\n",
    "    val_total_items = 0\n",
    "    # List to store all predictions\n",
    "    all_train_pred = []\n",
    "    all_val_pred = []\n",
    "    for x_batch, y_batch, val_x_batch, val_y_batch in train_val_loader: \n",
    "        # Get groundtruth in shuffle order\n",
    "        if t == 0:\n",
    "                train_y_ordered += y_batch\n",
    "                val_y_ordered += val_y_batch\n",
    "        # Clear stored gradient\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Init hidden state\n",
    "        model.init_hidden(batch_size=x_batch.size(0))\n",
    "        \n",
    "        # Training: Predict and calc loss\n",
    "        train_pred = model(x_batch)\n",
    "        loss = l1_loss(train_pred.view(len(train_pred), -1), y_batch)\n",
    "        all_train_pred.append(train_pred)\n",
    "        \n",
    "        # Tracking mean loss across batches\n",
    "        epoch_loss += train_pred.shape[0]*loss.item()\n",
    "        total_items += train_pred.shape[0]\n",
    "        \n",
    "        \n",
    "        # Validation: predict and calc loss\n",
    "        val_pred = model(val_x_batch)\n",
    "        val_loss = l1_loss(val_pred.view(len(val_pred), -1), val_y_batch)\n",
    "\n",
    "        all_val_pred.append(val_pred)\n",
    "\n",
    "        # Tracking mean loss across batches\n",
    "        val_epoch_loss += val_pred.shape[0]*val_loss.item()\n",
    "        val_total_items += val_pred.shape[0]\n",
    "\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "\n",
    "    # LR decay\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = epoch_loss/total_items\n",
    "    val_epoch_loss = val_epoch_loss/val_total_items\n",
    "    loss_history.append(epoch_loss)\n",
    "    val_loss_history.append(val_epoch_loss)\n",
    "    if t%10==0:\n",
    "        print(\"Epoch \", t, \"Train Loss: \", epoch_loss, \"Val Loss:\", val_epoch_loss, \"LR: \", optimiser.param_groups[0][\"lr\"])\n",
    "        \n",
    "    if np.isnan(epoch_loss):\n",
    "        break\n",
    "        \n",
    "    # Early stopping\n",
    "    if epoch_loss < best_loss:\n",
    "        i = 0\n",
    "        # Save best loss, predictoins, and hidden state\n",
    "        best_loss = epoch_loss\n",
    "        best_train_pred = torch.cat(all_train_pred.copy(), dim=0)\n",
    "        best_val_pred = torch.cat(all_val_pred.copy(), dim=0)\n",
    "    if (i > early_stopping):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_y_ordered, best_train_pred.detach().numpy().flatten())\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':best_train_pred.detach().numpy().flatten(), 'True':train_y.flatten()}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(val_y_ordered, best_val_pred.detach().numpy().flatten())\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':best_val_pred.detach().numpy().flatten(), 'True':val_y.flatten()}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playa_venv",
   "language": "python",
   "name": "playa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
