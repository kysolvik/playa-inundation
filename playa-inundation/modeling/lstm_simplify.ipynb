{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Robert Guthrie https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "# And: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import json\n",
    "import zoib\n",
    "\n",
    "np.random.seed(55)\n",
    "torch.manual_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(df):\n",
    "    ind_year = np.where(np.array(df.index.names)=='year')[0][0]\n",
    "    train_df = df.loc[df.index.get_level_values(ind_year)<=2010]\n",
    "    val_df = df.loc[(df.index.get_level_values(ind_year)>2010) & (df.index.get_level_values(ind_year)<=2014)]\n",
    "    test_df = df.loc[df.index.get_level_values(ind_year)>2014]\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def split_by_playa(x, y, seq_length):\n",
    "    seq_starts = np.arange(0, x.shape[0], seq_length)\n",
    "    x_arr = np.array([np.array(x[i:(i+seq_length)]) for i in seq_starts])\n",
    "    ids_arr = np.array([np.repeat(j, seq_length) for j in range(seq_starts.shape[0])])\n",
    "    y_arr = np.array([np.array(y[i:(i+seq_length)]) for i in seq_starts])\n",
    "    return x_arr, ids_arr, y_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of playas from 1 to 8\n",
    "num_playas = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = pd.read_csv('./prepped_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv and set to appropriate number of playas\n",
    "traj = pd.read_csv('./prepped_8.csv')\n",
    "traj = traj.loc[traj['new_id'].isin(range(num_playas))]\n",
    "\n",
    "# Set indices\n",
    "traj.set_index(['year','month','id'], inplace=True)\n",
    "\n",
    "# Drop new_id (used for embedding, but we're not doing that)\n",
    "traj.drop(columns=['new_id'], inplace=True)\n",
    "\n",
    "# Here's a chance to drop everything else, if you'd like\n",
    "# traj = traj[['precip', 'temp', 'vpd', 'acres', 'new_id', 'inundation']]\n",
    "\n",
    "# Drop everything that's all zeros\n",
    "traj.drop(columns=traj.columns[(traj.sum(axis=0)==0)], inplace=True)\n",
    "\n",
    "# Plot inundation\n",
    "traj['inundation'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params to set\n",
    "hidden_dim = 64\n",
    "embedding_dim = 4\n",
    "num_layers=1\n",
    "learning_rate = 0.05\n",
    "num_epochs = 300\n",
    "weight_decay = 0\n",
    "lr_gamma = 0.25\n",
    "lr_decay_step_size = 75 # Set this high if not needed\n",
    "regularization_weight = 0\n",
    "batch_size = 8\n",
    "\n",
    "early_stopping=100 # Set high if not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train, val, test = split_train_test_val(traj)\n",
    "train_X, train_y = train.values[:, :-1], train.values[:, -1]\n",
    "val_X, val_y = val.values[:, :-1], val.values[:, -1]\n",
    "test_X, test_y = test.values[:, :-1],  test.values[:, -1]\n",
    "\n",
    "# Run scaler\n",
    "train_X[:,:] = scaler.fit_transform(train_X[:,:])\n",
    "val_X[:,:] = scaler.transform(val_X[:,:])\n",
    "test_X[:,:] = scaler.transform(test_X[:,:])\n",
    "\n",
    "lstm_input_size = train_X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into arrays by id\n",
    "train_X_array, train_ids, train_y_array = split_by_playa(train_X, train_y, seq_length=int(train.shape[0]/num_playas))\n",
    "val_X_array, val_ids, val_y_array = split_by_playa(val_X, val_y, seq_length=int(val.shape[0]/num_playas))\n",
    "test_X_array, test_ids, test_y_array = split_by_playa(test_X, test_y, seq_length=int(test.shape[0]/num_playas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_ds = torch.utils.data.TensorDataset(\n",
    "    torch.Tensor(train_X_array), torch.Tensor(train_ids).long(), torch.Tensor(train_y_array),\n",
    "    torch.Tensor(val_X_array), torch.Tensor(val_ids).long(), torch.Tensor(val_y_array))\n",
    "train_val_loader = torch.utils.data.DataLoader(train_val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim, num_playas, output_dim=1,\n",
    "                    num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_playas = num_playas\n",
    "        \n",
    "        # Define embedding layer\n",
    "        self.embedding = nn.Embedding(self.num_playas, self.embedding_dim)\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim + self.embedding_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "\n",
    "        # Define activations for output\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.exp = torch.exp\n",
    "    \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        self.h = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        self.c = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        return\n",
    "\n",
    "    def forward(self, input, playa_ids):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of input: [batch_size, timesteps, input_dims]\n",
    "        # shape of lstm_out: [batch_size, timesteps, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        # Shape of y_pred: [batch_size, timesteps, 4]\n",
    "        # Run ids through embedding layer        \n",
    "        # Concat and run through LSTM\n",
    "        \n",
    "        \n",
    "        # Check that hidden layers have expected shape\n",
    "        assert self.h.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "        assert self.c.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "\n",
    "        # Run ids through embedding\n",
    "        self.emb_layer = self.embedding(playa_ids)\n",
    "\n",
    "        # Concat embedding and inputs and run through LSTM\n",
    "        lstm_out, (self.h, self.c) = self.lstm(torch.cat((input, self.emb_layer), 2), (self.h, self.c))\n",
    "        \n",
    "        # Assert that shapes are still as expected\n",
    "        assert self.h.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "        assert self.c.shape == torch.Size([self.num_layers, input.size(0), self.hidden_dim])\n",
    "        assert lstm_out.shape == torch.Size([input.size(0), input.size(1), self.hidden_dim]) # batch, seq_len, hidden\n",
    "\n",
    "        \n",
    "        # Run activation and get outputs\n",
    "        lin_act = self.linear(lstm_out)\n",
    "        y_pred = torch.cat((self.sigmoid(lin_act[:,:,0:2]),torch.exp(lin_act[:,:,2:4])), 2)\n",
    "\n",
    "        # Check that outputs are expected shape [batch_size, seq_len, 1]\n",
    "        assert lin_act.shape == torch.Size([input.size(0), input.size(1), 4])\n",
    "        assert lin_act.shape == y_pred.shape\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = LSTM(input_dim = lstm_input_size,\n",
    "             hidden_dim=hidden_dim,\n",
    "             embedding_dim=embedding_dim,\n",
    "             num_playas=num_playas,\n",
    "             output_dim=4,\n",
    "             num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=lr_decay_step_size, gamma=lr_gamma)\n",
    "#####################---------------------------------------------------------------------------\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "best_loss = 1000\n",
    "train_y_ordered = np.array([])\n",
    "val_y_ordered = np.array([])\n",
    "for t in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    val_epoch_loss = 0\n",
    "    total_items = 0\n",
    "    val_total_items = 0\n",
    "    # List to store all predictions\n",
    "    all_train_pred = []\n",
    "    all_val_pred = []\n",
    "    for x_batch, ids_batch, y_batch, val_x_batch, val_ids_batch, val_y_batch in train_val_loader: \n",
    "        \n",
    "        # Get groundtruth in shuffle order\n",
    "        if t == 0:\n",
    "                train_y_ordered = np.concatenate([train_y_ordered, y_batch.view(-1).detach().numpy()])\n",
    "                val_y_ordered = np.concatenate([val_y_ordered, val_y_batch.view(-1).detach().numpy()])\n",
    "                \n",
    "                \n",
    "        # Clear stored gradient\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Init hidden state\n",
    "        model.init_hidden(batch_size=x_batch.size(0))\n",
    "        \n",
    "        # Training: Predict and calc loss\n",
    "        train_pred = model(x_batch, ids_batch)\n",
    "        \n",
    "        loss = zoib.zoib_loss(\n",
    "            train_pred,\n",
    "            y_batch \n",
    "        )\n",
    "        \n",
    "        all_train_pred.append(train_pred.view(-1, 4))\n",
    "        \n",
    "        # Tracking mean loss across batches\n",
    "        epoch_loss += train_pred.shape[0]*loss.item()\n",
    "        total_items += train_pred.shape[0]\n",
    "        \n",
    "        \n",
    "        # Validation: predict and calc loss\n",
    "        val_pred = model(val_x_batch, val_ids_batch)\n",
    "        \n",
    "        val_loss = zoib.zoib_loss(\n",
    "            val_pred,\n",
    "            val_y_batch\n",
    "        ).float()\n",
    "        all_val_pred.append(val_pred.view(-1, 4))\n",
    "\n",
    "        # Tracking mean loss across batches\n",
    "        val_epoch_loss += val_pred.shape[0]*val_loss.item()\n",
    "        val_total_items += val_pred.shape[0]\n",
    "\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "\n",
    "    # LR decay\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = epoch_loss/total_items\n",
    "    val_epoch_loss = val_epoch_loss/val_total_items\n",
    "    loss_history.append(epoch_loss)\n",
    "    val_loss_history.append(val_epoch_loss)\n",
    "    if t%10==0:\n",
    "        print(\"Epoch \", t, \"Train Loss: \", epoch_loss, \"Val Loss:\", val_epoch_loss, \"LR: \", optimiser.param_groups[0][\"lr\"])\n",
    "        \n",
    "    if np.isnan(epoch_loss):\n",
    "        break\n",
    "        \n",
    "    # Early stopping\n",
    "    if epoch_loss < best_loss:\n",
    "        i = 0\n",
    "        # Save best loss, predictoins, and hidden state\n",
    "        best_loss = epoch_loss\n",
    "        best_train_pred = torch.cat(all_train_pred, dim=0)\n",
    "        best_val_pred = torch.cat(all_val_pred, dim=0)\n",
    "    if (i > early_stopping):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_loss)\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoib_expected(t):\n",
    "    # E = q*(1-p) + (1-p-q)*(conc1/(conc1+conc0))\n",
    "    # Or # = prob_1_given_not0*(1-prob_0) + (1 - prob_bernoulli)*(expect_val_beta)\n",
    "    t = t.detach().numpy()\n",
    "    prob_1 = t[:,1]*(1-t[:,0])\n",
    "    prob_beta = (1 - t[:,0])*(1 - t[:,1])\n",
    "    beta_expected = t[:,2]/(t[:,3]+t[:,2])\n",
    "    return prob_1 + prob_beta*beta_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_y_ordered, zoib_expected(best_train_pred))\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':zoib_expected(best_train_pred), 'True':train_y_ordered}).plot(xlim=[300,450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(val_y_ordered, zoib_expected(best_val_pred))\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':zoib_expected(best_val_pred), 'True':val_y.flatten()}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.DataFrame(best_train_pred.detach().numpy())\n",
    "param_df.columns = ['p','q','conc1','conc0']\n",
    "pd.plotting.scatter_matrix(param_df)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playa_venv",
   "language": "python",
   "name": "playa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
